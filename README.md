- An even simpler, toyer task for tokenized thoughts: tokenized autoencoders:
    - I settled on the 'blind' modification to the tokenized thoughts model.
        - This is where we have 2 separate models. One is the thinking policy, one is the answering policy.
        - The thinking policy sees the question and all previous though tokens, and produces another thought token.
        - The answering policy, given a chain of thought (and not the original context!) produces a final answer.
    - This is actually quite similair then to an autoencoder. We need to push some info through a bottleneck so that it can be recovered (or something else  can be done with the info) on the other side.
    - The biggest difference here is that the bottleneck is tokenization, forcing the outputs to be discrete, rather than continuous.
        - This causes a huge problem becuase tokenization is not differentiable!
        - In a normal AE, we can differentiate from the loss to the intermediate result, through to the weights that produced the intermediate result.
        - In this AE, this is not possible, so the reparametrization trick is used.
            - We simply take the reconstruction loss as a detached scalar value, and multiply it by the -logprob of producing the intermediate thinking tokens that were produced. This allows us to use the reconstruction loss to train the encoder given a particular tokenized intermediate result that it produced.
    - Specific test scenario: The models are given a token and tasked with simply recovering and outputting it. Specifically:
        - The input number (token, whatever) lies between 0 and M.
        - The thinking policy is given this and autoregressively produces T thinking tokens, having L_t different thinking tokens to choose from.
        - The answering policy is given the string of T thinking tokens, and prodces a distribution over possible input tokens.
        - The answering policy's loss is simply the logprob on the correct input token. Normal reconstruction loss.
        - But since the thinking policy is a separate model, it also needs its own loss function which takes in the string of thinking tokens.
        - Here is the reparameterization trick. We can differentiate with respect to the logprobs of the thinking tokens that it emitted, multiplied by how well the answering policy was able to reconstruct based on them.
    - You can imagine a few qualitatively different scenarios here based on the hyperparameters.
        - If M == T, the model simply needs to learn some 1 to 1 coding scheme where a single thinking token corresponds to a single input.
            - if the input is x, thinking policy outputs token y. If the answering policy sees thinking token y, the input was x.
        - If M < T, same as above except multiple thinking tokens could represent the same answer.
        - If M > T however, we now have a real information bottleneck. One thinking token cannot possible encode all the necessary information. More complex coding schemes are required to cross the gap.
            - Depending on M, T, and L_t, perfect reconstruction may be impossible.
                - Specifically, if log base T of M <= L_t, perfect reconstruction is possible.
    - Starting with the M == T scenario, all the model has to learn is a 1-1 mapping. The issue turns out to be the gradient flow.
        - The hurdle here is steep: both model's need to learn eachother's coding scheme based solely on their outputs.
            - The answering policy is getting updated to follow what the thinking policy is doing, and vice versa.
                - If we apply small biases towards a particular mapping in the logits when we sample thinking tokens, the answering model picks up on it, and the thinking policy accordingly picks up on what the answering model is learning, etc.
        - Of course at the start both know nothing, but random weight init provides some initial tendencies in the distributions of the thinking policy.
            - In practice this only gives us the smallest boost in accuracy over random chance. It seems there are few tendencies strong enough to get canonized.
        - What is needed here is some spontaneous symmetry breaking. If either model just draws out of a hat and picks some random association and sticks to it, the hill is exceedingly easy for the other model to follow.
            - So how do we escape the noise regime?

- naming conventions:
    - *add*: training a model for the toy task of modular addition
    - *think*: uses thinking tokens
        - *fixed*: instead of stopping thought rollouts once the model produces an end thought token, always sample a pre-set number of thinking tokens. runs about 20x faster.
        - *blind*: produce thinking tokens like normal. But to actually produce the next token prediction, the model can only see the thinking tokens, not the question. forces thinking tokens to contain useful information.
        - *super(vised)*: Here, instead of training for the next token prediction task (the non-thinking task) using the sampled thought tokens from the model, we use manually generated thinking tokens. Isolates training difficulties to the RL part.
            - This just makes it so that right from the start there is a 'correct' chain of thought for each question.
                - This avoids the need to 'invent a language' to communicate to the answering policy. Under 'super' variant, a language exists and the thinking policy just needs to learn it.
            - *clean*: the rl loss signal is hand crafted in some way. Probably giving 0 reward to wrong reasoning traces, positive constant to others.
        - *search*: search is used in the process of exploring the possible chains of thought during training, as opposed to epsilon-greedy exploration or no exploration.

- blind + super + clean works for max=100. + search also works but is not necessary.
    - The thinking policy does steadily learn the correct thinking tokens to output when given a clean loss signal only perfectly correct outputs. not so surprising.
    - So what should we loosen first?
        - my inclination is to figure out how to do rewards properly. So that would be getting rid of 'clean'

    - same combo does not work for 1000. The prediction policy can't even learn with supervised thinking sequences? bug?
        - Not bug, just had bad hyperparams apparently. need large batch sizes >= 64 and weight decay is evil.

    - removing clean also technically works beucase you can just softmax to recover the clean reward signal.
        - But this basically only works when the answering model knows whats going on, and probably only works in domains where perfect answers exist (so modular addition, but not general text prediction)
            - Although final answer logits in normal LMs are quite sparse/spiky. So not a single correct answer like arithmetic tasks, but close to one hot. few-hot. Could still be applicable on normal language.

    - Attempts to remove 'super':
        - Hard. Both models seem to converge to a uniform distribution.
        - Tried freezing the answer model to capture some noise/structure for the thinking model to pick up on, but nothing.
        - A rephrasing of the difficulty here: we need these models to invent a language to talk to each other with.
            - At first each model knows nothing. Ignoring noise, they basically have no association between inputs and outputs.
            - If one model is speaking language, the other can learn it easily. So how to get them to invent a language the other can learn?
            - The reason an autoencoder can do  this is beucase gradients flow from output, through the decoder to intermediate result, through the encoder to the inputs.

- This training method has several compounding levels of training difficulty
    - boostrapping problem. We have to learn to produce useful thinking tokens and learn to use thinking tokens simultaneously.
        - search
        - separate the thinking policy and the predicting policy
    - noisy/sparse rewards. The signal for the thinking token rl will be fairly noisy, even when the answer producing policy is fully trained.
        - train a value model
    - potentially large action space
        - search

- is modular addition just a bad task? I think so...
    - The thing I want to get at is algorithmic complexity.
        - As in normal transformers are limited in the complexity (# steps) of the algorithms they can learn to apply
        - thinking tokens allow unsupervised splitting over arbitrary length algorithms.
    - But is modular addition even complexity bounded? It seems like no?
        - higher  additions werent working until i tweaked hyperparams and raised d_model.
        - Seems more like bandwidth, information bottleneck squeeze limited.
        - I guess that's one of the key surprises of transformers: wide algorithms (memorization, heuristics, shortcuts) get you quite far!
        - You need to actually differentiate between naive complexity (how many operations are needed), and serial complexity.
            - Modular addition, as the grokking paper teaches us, is highly parallelizable, making the input size limit a matter of resid stream info capacity.
    - So perhaps a different task?
        - graph search task? encode graphs as lists of nodes or something?
        - Multiple modular additions in a single problem?
        - simple sat problems?
        - list sorting?
        - maybe no more toy problems and just do the thing?
    - But idk, isnt the whole point that it might learn an alternative strategy? 
        - Like if it's in the bandwidth bottlenecked regime on addition and has the opportunity to split its computation among multiple steps and it can't learn to do that, then the thing just doesn't work.

- So what sub-problem am I currently tackling?
    - We are focusing on fixed: + split + blind + super.
        - fixed number of  thinking tokens
        - answering and thinking policies are split; separate models
        - the thinking policy sees full context and produces a chain of thought. The answering policy only sees the chain of thought, not any other context.
        - and the are training the answering policy with a predetermined chain of thought sceme which the thinking policy must learn to reproduce based on reward signals.
    - We know the issue is not ONLY failure of signal between the models, becuase even with direct supervised training on correct rollouts (not doing it through rl), the model fails for max>>100.
    - Investiagtion reveals that this is due to the predetermined rollout scheme we chose, which was having 10 different thought tokens and using them to spell out the answer in digits.
        - This alleviates much of the difficulty, but not all!
        - For the least sig digit the thinking policy need not consider carries. Simple mod10 addition.
        - The thinking policy now must predict the second least sig digit given the first. just becuase we know the least sig digit of the sum does not tell us wether there was carry for free!
        - For the second least sig, we have to both do the mod10 for this digit and find the carry of the first digit, given its post carry value.
        - Etc. for max=500 we see the first thought token rollout be near perfect accuracy, the second be also entirely accurate but 10x less so than the first (in logprob terms)
            - And for the last it clearly has some idea of what's going on but is rarely confident and often confidently wrong.
        - However if i double all the dimensions of the model (but dont change the number of layers), it instead remains high entropy for the least sig digit, but learns the later 2 perfectly.

- AEs work beucase gradients flow from the output, decoder, latent representation, encoder, and inputs.
    - All the approaches here don't have that property becuase tokenization severs the gradients.
    - Maybe this suggests we should focus on non-tokenized, continuous thoughts? cuz differentiable?
        - Does this not matter? Am I already basically doing this through REINFORCE reparameterization trick?
    - And just to check, a single normal model which has an attn mask to the question tokens wouldn't work, right?
        - ?